{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  Ingerindo o arquivo estadios ###\n",
    "\n",
    "Objetivo: realizar um processo de ETL e disponibilizar os dados em uma camada apropriada\n",
    "\n",
    "Ler -> Transformar -> Escrever (ETL)\n",
    "\n",
    "1. Ler o arquivo csv do Data Lake\n",
    "2. Renomear as colunas para português utilizando o snake case\n",
    "3. Definir o esquema de dados correto\n",
    "4. Precisa ter uma coluna com a data em que o arquivo foi consumido\n",
    "5. Salvar os dados em formato parquet na camada silver\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo o arquivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_arquivo = \"estadios\"\n",
    "caminho_arquivo = f\"/mnt/datalake/bronze/{nome_arquivo}.csv\"\n",
    "\n",
    "estadios_df = spark.read\n",
    ".option(\"encondig\", \"UTF-16\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(caminho arquivo)\n",
    "\n",
    "display(estadios_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar o schema do dataframe criado\n",
    "estadios_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomear as colunas para português\n",
    "\n",
    "estadios_df_renomeado = estadios_df \\\n",
    "    .withColumnRenamed(\"id\", \"id_estadio\") \\\n",
    "    .withColumnRenamed(\"name\", \"nm_estadio\") \\\n",
    "    .withColumnRenamed(\"address\", \"endereco\") \\\n",
    "    .withColumnRenamed(\"city\", \"nm_cidade\") \\\n",
    "    .withColumnRenamed(\"country\", \"nm_pais\") \\\n",
    "    .withColumnRenamed(\"capacity\", \"vl_capacidade\") \\\n",
    "    .withColumnRenamed(\"surface\", \"tp_grama\") \\\n",
    "    .withColumnRenamed(\"image\", \"link_imagem\")\n",
    "\n",
    "display(estadios_df_renomeado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incluir a coluna de data de execução do script\n",
    "\n",
    "estadios_df_data = estadios_df_renomeado.withColumn(\"dt_ingestao\", current_timestamp())\n",
    "display(estadios_df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essa parte escreve o arquivo formato parquet na pasta silver\n",
    "# Se o arquivo já existir, ele irá sobrescrever\n",
    "estadios_df_data.write.mode(\"overwrite\").parquet(\"/mnt/datalake/silver/estadios\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
